<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Vid2Robot is an end-to-end video-conditioned robot policy using Cross attention.">
  <meta name="keywords" content="Video, robot, learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .video-container {
      display: inline-block;
    }

    .video-title {
      position: relative;
      color: white;
      /* Text color */
      font-size: 12px;
      /* Font size */
      /* Font weight */
      background-color: rgba(0, 0, 0, 0.75);
      /* Semi-transparent background */
      padding: 5px 10px;
      /* Padding */
      font-family: Arial, sans-serif;
      width: auto;
      height: auto;
    }


    .carousel-item {
      cursor: pointer;
      width: auto;
      height: 100%;
      object-fit: cover;
      max-width: 100%;
      max-height: 80%;
    }

    .lightbox {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.7);
      text-align: center;
    }
  </style>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://vidhijain.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
      </div>
    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Vid2Robot: End-to-end Video-conditioned Policy Learning with
              Cross-Attention Transformers</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://vidhijain.github.io">Vidhi Jain</a><sup>1, 2</sup></span>
              <span class="author-block">
                <a href="https://jmattarian.com/">Maria Attarian</a><sup>1, 3</sup></span>
              <span class="author-block">
                <a href=""> Nikhil J Joshi</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Ayzaan Wahid</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Danny Driess</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Quan Vuong</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="">Pannag R Sanketi</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Pierre Sermanet</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Stefan Welker</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Christine Chan</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="">Igor Gilitschenski</a><sup>3</sup>
              </span>
              <br>
              <span class="author-block">
                <a href="">Yonatan Bisk</a><sup>2</sup>
              </span>
              <span class="author-block">
                <a href="">Debidatta Dwibedi</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Google DeepMind</span>
              &nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Carnegie Mellon University</span>
              &nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>3</sup>University of Toronto</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="./vid2robot.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.m4v" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">Vid2Robot</span> understands the task from videos and can perform in unseen settings.
        </h2>
      </div>
    </div>
  </section>



  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" class="carousel-item" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_0_human_3p_observe_human_close_middle_drawer-m2rtccmoreframes_Thu Jan 25 11_09_42 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Close middle drawer</div>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_0_phase4_move_orange_near_blue_chip_bag-m2rtccmoreframes_Thu Jan 25 13_36_05 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Move orange near blue chip bag</div>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_25_human_3p_observe_human_open_middle_drawer-m2r_tccmoreframes_fc_Tue Jan 30 10_21_22 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Open middle drawer</div>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_13_human_3p_observe_human_pick_green_rice_chip_bag-m2r_tccmoreframes_fc_Tue Jan 30 08_46_35 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Pick green rice chip bag</div>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_2_phase4_move_green_jalapeno_chip_bag_near_green_rice_chip_bag-m2rtccmoreframes_Thu Jan 25 13_13_22 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Move green jalapeno chip bag near green rice chip bag</div>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_8_human_3p_observe_human_move_green_jalapeno_chip_bag_near_coke_can-bcz_human_robot_mix_Tue Jan 30 12_58_14 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Move green jalapeno chip bag near coke can</div>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source
                src="./static/videos/rollouts/success/with ref/rollout_with_ref_5_human_3p_observe_human_move_rxbar_chocolate_near_coke_can-m2r_tccmoreframes_fc_Tue Jan 30 12_40_25 2024.mp4"
                type="video/mp4">
            </video>
            <div class="video-title">Move rxbar chocolate near coke can</div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              While large-scale robotic systems typically rely on textual instructions for tasks,
              this work explores a different approach: <em>Can robots infer the task directly from observing humans?
              </em>
              This shift necessitates the robot's ability to decode human intent and
              translate it into executable actions within its own physical constraints and environment.
            </p>
            <p>
              We introduce <span class="dnerf">Vid2Robot</span>, a novel end-to-end video-based learning framework for
              robots.
              Given a video demonstration of a manipulation task and current visual observations,
              <span class="dnerf">Vid2Robot</span> directly produces robot actions.
              This is achieved through a unified representation model trained on a large dataset of human video and
              robot trajectory.
              The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state
              and generate appropriate actions that mimic the observed task.
              To further improve policy performance, we propose auxiliary contrastive losses
              that enhance the alignment between human and robot video representations.
            </p>
            <p>
              We evaluate <span class="dnerf">Vid2Robot</span> on real-world robots, demonstrating a 23% improvement in
              performance compared to other video-conditioned policies when using human demonstration videos.
              Additionally, our model exhibits emergent capabilities, such as successfully transferring observed motions
              from one object to another, and long-horizon composition, thus
              showcasing its potential for real-world applications.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <br>
      <hr><br>
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Architecture</h2>
          <div class="publication-video">
            <!-- <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
            <video id="architecture" autoplay controls muted loop playsinline height="80%">
              <source src="./static/videos/arch.mov" type="video/mp4">
            </video>
          </div>
          <div class="content has-text-justified">
            <p>
              Our policy takes as input the prompt video and the current robot state and outputs robot actions. It
              consists of four modules:
              (1) prompt video encoder (2) robot state encoder, (3) state-prompt encoder, and (4) robot action decoder.
              Our model takes as input frames of the prompt video and the robot's current observations,
              encodes those into prompt video and robot state token embeddings,
              which are then processed through into state-prompt encoder
              and decoded into a robot action for the current timestep.
            </p>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <br>
  <hr><br>


  <!-- Long Task Composition. -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="column is-centered">
        <div class="content has-text-centered">
          <h2 class="title is-3">Long Horizon Task Composition</h2>
        </div>
        <div class="content has-text-justified">
          <p>
            We can solve long horizon task compositions by combining prompt videos. Here we showcase long horizon
            transfer to:
          </p>
          <section class="hero is-light is-small">
            <div class="hero-body">
              <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                  <div class="item">
                    <video class="carousel-item" id="matting-video" autoplay controls muted loop playsinline
                      height="100%">
                      <source src="./static/videos/long_horizon_tasks/open_place_close.mov"
                        type="video/mp4">
                    </video>
                    <div class="video-title">Prompt Videos show
                      (1)<em> Open the drawer</em>, (2)<em>Place the Coke can in it</em> and (3) Close the drawer.
                      Robot sees the <em>Coke can, Chips bag and Rxbar</em> in its view and first moves the chips bag
                      close to
                      the coke can
                      and then moves the rxbar near the coke can.
                    </div>
                  </div>
                  <div class="item">
                    <video class="carousel-item" id="matting-video" autoplay controls muted loop playsinline
                      height="100%">
                      <source
                        src="./static/videos/long_horizon_tasks/move_chips_near_can_then_move_rxbar_near_can.mp4"
                        type="video/mp4">
                    </video>
                    <div class="video-title">Prompt Videos show
                      (1)<em> Move Chips near Coke can</em> and (2)<em>Move Rxbar near Coke Can</em>.
                      Robot sees the <em>Coke can, Chips bag and Rxbar</em> in its view and first moves the chips bag
                      close to
                      the coke can
                      and then moves the rxbar near the coke can.
                    </div>
                  </div>
                  <div class="item">
                    <video class="carousel-item" id="matting-video" autoplay controls muted loop playsinline
                      height="100%">
                      <source src="./static/videos/long_horizon_tasks/knock_then_place_upright.mp4"
                        type="video/mp4">
                    </video>
                    <div class="video-title">Prompt Videos show
                      (1)<em> Knock pepsi can</em> and (2)<em> Place coke can
                        upright</em>.
                      Robot sees the <em>Green Soda can</em> in its view and executes knocking it over and then placing
                      it
                      upright.
                    </div>
                  </div>
                  <div class="item">
                    <video class="carousel-item" id="matting-video" autoplay controls muted loop playsinline
                      height="100%">
                      <source src="./static/videos/long_horizon_tasks/place_upright_then _knock.mp4"
                        type="video/mp4">
                    </video>
                    <div class="video-title">Prompt Videos show
                      (1)<em> Place Coke can upright</em> and (2)<em> Knock Pepsi can over</em>.
                      Robot sees the <em>Green Soda can</em> in its view and executes placing it
                      upright and then knocking it over.
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>
        </div>
      </div>
      <!--/ Long Task Composition. -->

    </div>
  </section>
  
  <br>
  <hr><br>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{jain2024vid2robot,
  author    = {Jain, Vidhi and Attarian, Maria and Joshi, Nikhil J and Wahid, Ayzaan and Driess, Danny and Vuong, Quan and Sanketi, Pannag R and Sermanet, Pierre and Welker, Stefan and Chan, Christine and Gilitschenski, Igor and Bisk, Yonatan and Dwibedi, Debidatta},
  title     = {Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers},
  year      = {2024},
}</code></pre>
    </div>
  </section>
  <footer>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          This template was inspired from the <a href="nerfies.github.io">here</a>.
        </div>
      </div>
    </div>
  </footer>
</body>

</html>